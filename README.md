# 神经网络模型演进：从 CNN 到 Transformer


本项目旨在探索和实现三种不同架构的图像分类模型，展示了从经典卷积神经网络（CNN）到更现代的 Vision Transformer (ViT) 的技术演进。

这是一个为小组汇报和技术分享专门准备的指南，目标不是复现代码，而是用最直白的语言解释：

它是什么？ (What)

它解决了什么问题？ (Why)

它是怎么做的？ (How)

## 🚩 第一部分：我们的“基础工具箱”（所有模型通用）

在开始构建任何模型之前，我们需要一套标准的“工具”。在 basic_model.py 中，我们定义了所有实验通用的数据处理流程和训练策略。

**1. 数据增强 (Data Augmentation)**


它是什么？
transforms.Compose([...]) 里的所有操作，比如随机旋转、裁剪、翻转。

它解决了什么问题？
过拟合 (Overfitting)。想象一下，如果模型“背诵”了所有训练图片（比如“记住”了某张猫图的背景），它在考试（测试集）时就无法识别新图片里的猫。

它是怎么做的？
我们通过“数据增强”来“作弊”。在每次喂给模型数据时，我们都对图片进行轻微的修改（比如旋转 15 度、左右翻转）。这样一来，模型永远不会看到两张完全一样的图片。

效果： 模型被迫去学习“猫”的本质特征（比如“有毛的尖耳朵”），而不是去记某张图片的特定像素。

代码中的体现：

transforms.RandomCrop(32, padding=4)：随机裁剪（模拟“凑近看”或“平移”）。

transforms.RandomHorizontalFlip()：随机水平翻转（“左边的猫”和“右边的猫”都是猫）。

**2. 训练策略 (Training Strategy)**

光有模型还不够，我们如何“指挥”模型学习，是决定模型好坏的关键。

*2.1 优化器 (Optimizer): AdamW*

它是什么？
指挥模型更新参数（权重）的“引擎”。

它解决了什么问题？
传统的“梯度下降”(SGD) 就像一个盲人下山，步子迈多大（学习率）全靠猜。AdamW 是一个“智能引擎”。

它是怎么做的？

Adam： 它结合了两种“惯性”。1) 动量 (Momentum)：如果连续几次都往一个方向走，说明这个方向很可能是对的，那就“加速”；2) 自适应 (Adaptive)：给每个参数都配一个单独的学习率，重要的参数（如边缘特征）就“慢点调”，不重要的参数就“快点调”。

W (Weight Decay)： AdamW 是 Adam 的一个改良版。它通过“权重衰减”来惩罚那些变得过大的参数。这是一种防止“过拟合”的绝佳手段，强迫模型用更“简约”的方式去解决问题。

*2.2 学习率调度 (LR Scheduler): CosineAnnealingLR*

它是什么？
一个“自动油门”控制器。

它解决了什么问题？
在训练刚开始时，我们离“正确答案”还很远，我们希望步子迈大一点（大学习率），跑得快一点。当训练快结束时，我们已经很接近答案了，这时需要慢慢来，精细调整（小学习率），以免“跑过头”。

它是怎么做的？
CosineAnnealingLR（余弦退火）就是让学习率按照“余弦函数”的曲线平滑地下降。从一开始的大步子，平滑过渡到最后的小碎步，实现“快又稳”的收敛。

*2.3 早停机制 (Early Stopping*

它是什么？
一个“智能刹车系统”。

它解决了什么问题？
训练并不是越久越好。当模型在训练集上的准确率还在上升，但在测试集（它没见过的题）上的准确率（val_acc）却开始下降时，就说明模型已经开始“过拟合”（背答案）了。

它是怎么做的？
我们的 EarlyStopping 类会一直盯着测试集的准确率 (val_acc)。

如果 val_acc 连续 patience=5 个回合都没有创下新高，我们就认为模型已经达到了它的极限。

这时系统会立即停止训练，节省时间。

最关键的是 restore_best=True：系统会自动回滚，加载并保存那个“历史最佳”的模型权重，而不是用最后一次（可能已经过拟合）的权重。

## 🚩 第二部分：模型架构的演进

现在，我们来看看三种不同的模型是如何一步步解决更复杂问题的。

演进 1：Simple4LayerCNN (朴素卷积网络)

这是 basic_model.py 中实现的基础模型。

核心思想：层级特征 (Feature Hierarchy)
CNN 模仿人眼看东西的方式：从点到线，再到面，最后到物体。

它是怎么做的？

*nn.Conv2d (卷积层)：“特征放大镜”。*

想象你有很多个“放大镜”（称为“卷积核 Kernel”）。

第一个放大镜（比如 conv1 里的 32 个之一）专门在图上找“横线”。

第二个放大镜专门找“竖线”。

第三个专门找“绿色”。

conv1 (3 -> 32 通道)：就是用 32 个不同功能的“放大镜”把整张图扫描一遍，得到 32 张“特征图”（比如一张“横线图”、一张“竖线图”等）。

*nn.BatchNorm2d (BN层)：“数据稳定器”。*

经过“放大镜”处理后，得到的“特征图”可能信号强度不一（有的特别亮，有的特别暗）。

BN 层的唯一作用就是把这些信号重新“拉回”到一个标准的、稳定的范围。

为什么重要？ 这是现代深度学习中最重要的组件之一。它极大地稳定了训练过程，让模型收敛得更快，没它基本训不动。

*nn.ReLU (激活函数)：“特征过滤器”。*

一个简单的规则：只保留正数（有用的特征），丢掉负数（没用的或抑制性的特征）。

它为模型引入了“非线性”——这是让模型能学会复杂组合（比如“尖耳朵”+“圆眼睛”=“猫”）的关键。

nn.MaxPool2d (池化层)：“信息压缩器”。

当“放大镜”找到太多特征时，信息就爆炸了，计算量也很大。

池化层的作用就是“总结”。它把一个小区域（比如 2x2）内的信息，只保留最强的那一个（“嘿，这块区域有‘横线’特征！”），然后把其他信息丢掉。

好处： 1) 极大减少计算量（图像尺寸减半）；2) 让模型更鲁棒（特征稍微挪动一下，不影响总结结果）。


3. classifier (全连接层)：最后的“决策者”。它查看所有收集到的“复杂特征”，然后“投票”决定这最可能是“汽车”还是“猫”。

**SimpleCNN 的局限性：网络退化**  
一个很自然的想法是：模型越深 = 提取的特征越高级 = 效果越好。
但 SimpleCNN 有个致命问题：当网络堆得非常深时（比如 50 层、100 层），性能不但不提升，反而会急速下降。这称为“网络退化” (Degradation)。

为什么？ 想象信息要穿过 100 层“放大镜”，每过一层信息都会有损耗或扭曲（梯度消失/爆炸），到最后早就“面目全非”了，模型根本无法学习。

***演进 2：ResNet (残差网络)***

ResNet-18.py 引入了两个革命性的新思想：1. 残差连接 (思想) 和 2. 迁移学习 (策略)。

*2.1 核心思想：残差连接 (Residual Connection)*

它是什么？
也叫“跳层连接 (Skip Connection)”。ResNet (Residual Network) 的名字就来源于此。

它解决了什么问题？
完美解决了上一节提到的“网络退化”问题。

它是怎么做的？  
既然信息穿过“处理层”（比如 conv3）会丢失或扭曲，那我就修一条“近道 (Shortcut)”！

SimpleCNN 的逻辑： 输出 = 处理层(输入)

ResNet 的逻辑： 输出 = 处理层(输入) + 输入 (通过“近道”把原始输入原封不动地加回来)

大白话：

原始信息 输入 (X) 可以通过“近道”无损地传到很深的层。

“处理层”的负担变小了：它不再需要学习完整的 输出 (Y)，它只需要学习 输出 (Y) 和 输入 (X) 之间的**“差值” (Residual)**。

如果模型发现某一层什么都不做（即 差值=0）是最好的，它就可以很容易地让“处理层”的权重变为 0，输出 就能恒等于 输入。

好处： 梯度可以畅通无阻地在深层网络中传播。这使得网络可以变得极深（50 层、101 层、152 层）而不会退化，性能也随之飙升。

2.2 核心策略：迁移学习 (Transfer Learning)

它是什么？
models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) 这行代码的含义。

它解决了什么问题？
“冷启动” 和 “数据/算力不足” 的问题。

SimpleCNN 是“从零开始”学习的，它就像一个婴儿，需要海量的数据（比如 CIFAR-10 的 5 万张图）才能慢慢学会什么是“轮子”、什么是“毛发”。

如果我们数据量很少（比如只有 1000 张图），SimpleCNN 根本学不会。

它是怎么做的？
“站在巨人的肩膀上”。

“巨人”： 谷歌、Meta 等大公司，用超强的算力，在 ImageNet 数据集（包含 120 万张图，1000 个类别，如各种狗、车、生活用品）上训练了一个超强的 ResNet-18 模型。

“肩膀”： 这个训练好的模型 weights。这个模型已经是一个“视觉专家”了，它已经学会了如何识别“边缘”、“纹理”、“形状”等所有通用的视觉底层特征。

“我们”： 我们加载这个“巨人”的预训练权重 (weights=...)，它已经包含了所有通用的视觉知识。

*2.3 核心操作：微调 (Fine-Tuning)*

它是什么？
model.fc = nn.Linear(model.fc.in_features, 10) 这行代码的含义。

它解决了什么问题？
“巨人”虽然厉害，但它的“大脑” (最后一层 fc) 是用来做 ImageNet 的 1000 分类题的。我们的任务是做 CIFAR-10 的 10 分类题。

它是怎么做的？

“冻结”与“替换”： 我们把“巨人”的身体（所有前面的卷积层，即“视觉知识”）保留下来，因为它很有用。

我们扔掉“巨人”原来的“大脑”（最后一层 1000 分类 fc）。

我们换上一个我们自己的、全新的“大脑”：nn.Linear(..., 10)，一个只输出 10 个类别的分类层。

“微调”： 我们用 CIFAR-10 的数据去训练。因为“身体”已经很强了，我们只需要用一个很小的学习率，稍微“微调”一下参数，让模型适应我们的 10 分类任务即可。

*2.4 ResNet-18.py 中的其他技术细节*

[技术点] 图像尺寸适配 (Image Resizing)：

问题： “巨人”模型 (ResNet) 只吃 224x224 像素的图片，但 CIFAR-10 的图片只有 32x32。

解决： 我们在数据预处理 transforms 中，强行把 32x32 的小图**“放大”**到 224x224 (Resize / RandomResizedCrop)，以匹配模型的输入要求。

[技术点] 高级数据增强 (AutoAugment)：

升级： 我们不再使用 basic_model 里的“随机旋转/翻转”这种简单组合。

策略： 我们使用了 AutoAugmentPolicy.IMAGENET。这是一套被证明在 ImageNet 上效果极佳的**“增强组合拳”**（比如“先锐化 30% 再平移 10 像素”），我们直接“拿来主义”，用它来进一步提升模型的泛化能力。

[技术点] 标签平滑 (Label Smoothing)：

问题： 传统的交叉熵（CrossEntropyLoss）会迫使模型对正确答案给出 100% 的自信（[0, 0, 1, 0, ...]）。这很容易导致模型“过拟合”和“过于自信”。

解决： 我们使用了 label_smoothing=0.05。

大白话： 我们修改了“标准答案”。我们告诉模型，正确答案“猫”的标签不再是 1.0，而是 0.95；同时，其他错误答案（比如“狗”）的标签不再是 0.0，而是分摊剩下 0.05 的一点点概率。

好处： 这让模型在学习时变得更“谦虚”，降低了模型“过拟合”的风险，提高了模型的泛化能力。

**演进 3：Vision Transformer (ViT)**

VIT.py 引入了第三种，也是最具革命性的架构。它完全抛弃了 CNN 的“卷积”，而是从自然语言处理 (NLP) 领域“跨界”引入了 Transformer 架构。

它解决了什么问题？
CNN (包括 ResNet) 有一个“天生缺陷”：“卷积核”是局部的 (Local)。

无论 ResNet 堆得多深，它底层的 conv 始终只能看到 3x3 或 5x5 的小区域。它就像一个“近视眼”，必须一层层往上“总结”，才能“间接”地感知到全局信息。

这导致 CNN 在捕捉**“长距离依赖关系”**（比如“左上角的猫头”和“右下角的猫尾”之间的关系）时非常吃力。

它的核心思想 (Aha! Moment)：
全局注意力机制 (Global Self-Attention)。

大白话： 既然 CNN 是“近视眼”，那我们就干脆不用它。我们设计一个“远视眼”架构，从一开始就能看到图像的全局。

它是怎么做的？
ViT 的工作流和 CNN 截然不同：

“图像切块” (Image Patching)：

ViT 的第一步是不用卷积。它粗暴地把 224x224 的图像“剪”成 14x14=196 个小方块** (Patches)。

在我们的代码中，模型 google/vit-base-patch16-224-in21k 的名字就说明了：它使用 16x16 像素的方块。  
  其实讲通俗一点就是图片是224x224的（因为训练的时候是这个大小，如果输入图片不是这个大小就要先变成这个大小）。  
然后把224*224的图片分成16x16的小图片，一共分成14x14个。为什么分成16x16的呢，这就要参考谷歌的一篇很出名的论文：an image is worth 16x16 words  
通俗点就是人家试出来最好的就是这个数字了 。

“线性嵌入” (Patch Embedding)：

每个“小方块” (Patch) 都是一个 16x16x3 的像素块。模型会把这个小方块“压扁”成一个向量（一个“数字列表”），这个向量就代表了这个小方块的全部信息。

[关键] [CLS] 令牌 (Token)： 在把所有方块“打包”之前，模型会额外**“塞入”一个特殊的、可学习的“班长令牌” ([CLS] Token)。这个“班长”的任务就是“领导和总结”**。

“全局开会” (Transformer Encoder)：

所有“小方块”（加上“班长”）被一起送入一个“大型会议室”（Transformer 编码器）。

在这个“会议室”里，“注意力机制”开始工作：每一个方块都会和其他所有方块“互动”并打分。

举例： 位于“猫头”位置的方块，会“环顾四周”，发现它应该高度关注 (High Attention) “猫爪”和“猫尾”的方块，而忽略 (Low Attention) “背景草地”的方块。

好处： 这种机制让 ViT 从第一层开始就具有全局视野 (Global View)。它能捕捉到 CNN 难以企及的“长距离依赖关系”。

“班长做总结” (Classification)：

经过多轮“开会”（多层 Encoder）后，我们忽略所有“小方块”的最终输出。

我们只看那个“班长” ([CLS] Token) 的最终输出向量。

我们认为，在多轮“全局讨论”后，这个“班长”已经充分吸收了图像中所有关键信息，成为了整个图像的“精华总结”。

最后，我们把这个“总结向量”送入一个简单的“决策者”（全连接层），“投票”决定最终分类。

3.1 ViT.py 中的技术细节

[技术点] Hugging Face transformers 库：

我们没有像 SimpleCNN 那样自己搭模型，而是使用了 transformers 库（NLP 领域的事实标准）。

google/vit-base-patch16-224-in21k：这是一个由 Google 训练的、在超大规模 ImageNet-21k 数据集（比 ResNet 用的 ImageNet-1k 大得多）上预训练的“ViT 巨人”。

[技术点] ImageProcessor 和 collate_fn：

为什么？ ViT 这种“巨人”模型，对数据的“口味”非常挑剔。它要求**“绝对一致”**的归一化和格式化（即必须和它在 Google 训练时一模一样）。

processor： 这是 ViT 配套的“数据管家”。我们在 build_vit_model 时，连模型带“管家”一起下载。

collate_fn： 这是我们的“打包工”。

DataLoader 先把一批 PIL 图像（经过 train_tfms 增强）交给“打包工”。

“打包工” (collate_fn) 调用“数据管家” (processor)。

“管家”把这批图像**“打包”**成 ViT 唯一能“吃”的格式：pixel_values（一个特殊的张量）。

总结： 这是一个比 transforms.ToTensor() 更高级、更严格的数据转换流程。

[技术点] 冻结主干 (Freezing the Backbone)：

策略： for p in model.vit.parameters(): p.requires_grad = False

为什么？ 这个 ViT “巨人”太强了，它是在 21k 个类别上训练的，它的“视觉知识”已经非常渊博。
问题： 如果我们在 CIFAR-10 这种“小池塘”里让它“自由泳”（全部微调），它巨大的参数量会瞬间“过拟合”。

解决： 我们执行**“特征提取” (Feature Extraction)** 策略。

我们**“冻结”** (requires_grad = False) ViT 的整个“身体” (model.vit)。

我们只训练（或微调）我们新换上的那个“大脑”（classifier 层）。

大白话： 我们把“巨人”当做一个**“只看不动”的特征提取器**。我们让它看图，然后听它（[CLS] Token）的“总结”，我们只训练一个简单的“决策者”来理解它的“总结”并将其对应到 10 个类别。